
@online{4TiaoXiaoXiShiPinFenXiZhiSTFTDuanShiFuLiYeBianHuanDeYuanLiYuDaiMaShiXian,
  title = {(4条消息) {{时频分析之STFT}}：短时傅里叶变换的原理与代码实现（{{非调用Matlab API}}）\_frostime的博客-{{CSDN博客}}},
  url = {https://blog.csdn.net/frostime/article/details/106816373},
  urldate = {2021-03-10}
}

@online{4TiaoXiaoXiShiPinFenXiZhiSTFTDuanShiFuLiYeBianHuanDeYuanLiYuDaiMaShiXiana,
  title = {(4条消息) {{时频分析之STFT}}：短时傅里叶变换的原理与代码实现（{{非调用Matlab API}}）\_frostime的博客-{{CSDN博客}}},
  url = {https://blog.csdn.net/frostime/article/details/106816373}
}

@article{ajmeraSpeechMusicSegmentation2003,
  title = {Speech/Music Segmentation Using Entropy and Dynamism Features in a {{HMM}} Classification Framework},
  author = {Ajmera, Jitendra and McCowan, Iain and Bourlard, Hervé},
  date = {2003-05-01},
  journaltitle = {Speech Communication},
  shortjournal = {Speech Communication},
  volume = {40},
  pages = {351--363},
  issn = {0167-6393},
  doi = {10.1016/S0167-6393(02)00087-0},
  url = {https://www.sciencedirect.com/science/article/pii/S0167639302000870},
  urldate = {2021-02-17},
  abstract = {In this paper, we present a new approach towards high performance speech/music discrimination on realistic tasks related to the automatic transcription of broadcast news. In the approach presented here, an artificial neural network (ANN) trained on clean speech only (as used in a standard large vocabulary speech recognition system) is used as a channel model at the output of which the entropy and “dynamism” will be measured every 10 ms. These features are then integrated over time through an ergodic 2-state (speech and non-speech) hidden Markov model (HMM) with minimum duration constraints on each HMM state. For instance, in the case of entropy, it is indeed clear (and observed in practice) that, on average, the entropy at the output of the ANN will be larger for non-speech segments than speech segments presented at their input. In our case, the ANN acoustic model was a multi-layer perceptron (MLP, as often used in hybrid HMM/ANN systems) generating at its output estimators of the phonetic posterior probabilities based on the acoustic vectors at its input. It is from these outputs, thus from “real” probabilities, that the entropy and dynamism are estimated. The 2-state speech/non-speech HMM will take these two-dimensional features (entropy and dynamism) whose distributions will be modeled through multi-Gaussian densities or a secondary MLP. The parameters of this HMM are trained in a supervised manner using Viterbi algorithm. Although the proposed method can easily be adapted to other speech/non-speech discrimination applications, the present paper only focuses on speech/music segmentation. Different experiments, including different speech and music styles, as well as different temporal distributions of the speech and music signals (real data distribution, mostly speech, or mostly music), illustrate the robustness of the approach, always resulting in a correct segmentation performance higher than 90\%. Finally, we will show how a confidence measure can be used to further improve the segmentation results, and also discuss how this may be used to extend the technique to the case of speech/music mixtures. Résumé Dans cet article, nous présentons une nouvelle approche particulièrement performante de discrimination parole/musique dans le cadre d’applications réelles de transcription de nouvelles diffusées. Dans cette approche, un réseau de neurones artificiels (ANN) entraı̂né exclusivement sur de la parole claire (provenant d’un système standard de reconnaissance de la parole grand vocabulaire) est utilisé comme modèle de canal à la sortie duquel nous mesurons toutes les 10 ms l’entropie et le “dynamisme”. Ces caractéristiques sont alors intégrées dans le temps à l’aide d’un modèles de Markov caché (HMM) ergodique à deux états (parole et non-parole) incluant également des contraintes de durée minimum sur chaque état. Par exemple, dans le cas de l’entropie, il est effectivement clair (et observé en pratique) que l’entropie à la sortie du ANN sera en moyenne plus élevée pour des segments non-parole que des segments de parole présentés à son entrée. Dans notre cas, le modèle acoustique ANN est un perceptron multi-couche (MLP, comme souvent utilisé dans les systèmes hybrides HMM/ANN) générant à sa sortie des estimateurs de probabilités a posteriori de phonèmes étant donné les vecteurs acoustiques d’entrée. C’est à partir de ces sorties, et donc de “vraies” probabilités que l’entropie et le dynamisme sont estimés. Le modèle HMM parole/musique à deux états prends ensuite ces deux caractéristiques (entropie et dynamisme) dont les distributions sont modélisées par des densités multi-gaussiennes ou par un second MLP. Les paramètres de ce modèle HMM sont entraînés par un Viterbi supervisé. Bien que l’approche proposée ici puisse être facilement adaptée à d’autres applications de discrimination parole/non-parole, nous nous focalisons ici sur le problème de segmentation parole/musique. Différentes expériences, incluant différents styles de parole et musique, ainsi que différentes distributions temporelles des signaux de parole et musique (distributions réelles, surtout parole, ou surtout musique), illustrent la robustesse de l’approche qui résulte toujours en des performances de segmentation correcte supérieure à 90\%. Finalement, nous montrons comment l’utilisation d’un niveau de confiance peut améliorer les résultats de segmentation, et comment ceci peut être utilisé pour traiter les cas de mélanges de parole et musique.},
  file = {/Users/isaac/Zotero/storage/CPI4ICKK/Ajmera 等。 - 2003 - Speechmusic segmentation using entropy and dynami.pdf;/Users/isaac/Zotero/storage/73E3E8KK/S0167639302000870.html},
  keywords = {Audio segmentation,Dynamism,Entropy,GMM,HMM,MLP,Speech/music discrimination},
  langid = {english},
  number = {3}
}

@thesis{caoJiYuJiQiXueXiDeYinLeQingGanShiBieFangFaYanJiu2018,
  title = {基于机器学习的音乐情感识别方法研究},
  author = {曹, 智贤},
  date = {2018},
  institution = {{湖南大学}},
  url = {https://kns.cnki.net/KCMS/detail/detail.aspx?dbcode=CMFD&dbname=CMFD202001&filename=1019826622.nh&v=},
  abstract = {音乐在人类历史中起着重要的作用,在现今的数字时代更是如此。现在音乐的数量以指数的形式增长,与此同时,对音乐进行组织、分类、检索的需求也在不断增加。基于音乐情感的分类与检索有别于传统基于音乐文本的分类与检索方式,更加注重创作者的情感表达和音乐在心理学方面的独特特点,也是用户人群不可或缺的个性化需求,因此越来越受到关注。音乐情感识别就是按情感对音乐进行识别分类。本文分析了当今国内外对音乐情感识别的研究现状,并总结了已有的音乐情感识别研究中的情感模型、数据集、音乐特征、机器学习算法、系统框架,并根据这些特点选择使用机器学习的方式来进行音乐情感识别。随着人工智能技术迅猛发展,机器学习也迎来了研究热潮。机器学习的过程是通过现有数据训练模型,然后用训练的模型预测结果。机器学习让计算机模拟人学习的过程,通过经验知识对未知做出预测判断。本文选择使用经典的k邻近(k-Nearest Neighbor,KNN)回归与支持向量机(Support Vector Machine,SVM)算法进行维度和离散模型的音乐情感识别的实现。由于维度模型的不确定性更高,KNN回归算法效果欠佳,还有很大的改善空间,可以借助其他辅助算法或者其他音乐信息来提高识别准确性;同时,在实验中比较了近邻数对识别准确性的影响,邻近数过大或者过小都会使得识别表现变差,因此必须选择合适的邻近数。在离散模型中,SVM算法识别表现良好,在实验中比较了不同核函数下的识别效果,发现核函数的选择对识别结果的影响较大,想要获得较高的识别率必须要选择合适的核函数。最后,本文总结了目前通过机器学习进行音乐情感识别存在的问题,并对音乐情感识别的未来进行展望。},
  editora = {余, 小游 and 孙, 广富},
  editoratype = {collaborator},
  file = {/Users/isaac/Zotero/storage/A2LTETPE/基于机器学习的音乐情感识别方法研究_曹智贤.caj},
  keywords = {emotional representation,feature extraction,machine learning,music information retr ieval(MIR),情感表示,机器学习,特征提取,音乐信息检索(MIR) Music Emotion Recognition(MER),音乐情感识别(MER-Music Emotion Recognition)},
  langid = {中文;},
  type = {硕士}
}

@inproceedings{chenAMG1608DatasetMusic2015,
  title = {The {{AMG1608}} Dataset for Music Emotion Recognition},
  booktitle = {2015 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Chen, Yu-An and Yang, Yi-Hsuan and Wang, Ju-Chiang and Chen, Homer},
  date = {2015},
  pages = {693--697},
  publisher = {{IEEE}},
  file = {/Users/isaac/Zotero/storage/8XQBCW8U/Chen 等。 - 2015 - The AMG1608 dataset for music emotion recognition.pdf;/Users/isaac/Zotero/storage/4SCU2VD2/7178058.html},
  keywords = {AMG1608数据库}
}

@article{chenDaiYueShuDeJuLeiFenXi1999,
  title = {带约束的聚类分析},
  author = {陈, 峰 and 吴, 艳乔 and 祝, 绍琪 and 杨, 树勤},
  date = {1999},
  journaltitle = {数理医药学杂志},
  pages = {18--19},
  issn = {1004-4337},
  url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CJFD&dbname=CJFD9899&filename=SLYY902.008&v=2rtIdeHzNplVQ%25mmd2FU6VcjN%25mmd2BP4u68qMXPYAGM9j51SvX2XM7y4mO1dZhfQWiexgqG%25mmd2FE},
  urldate = {2021-01-20},
  abstract = {提出了一种带约束的聚类方法，又称条件系统聚类法，该法基于系统聚类的思想，在聚类过程中按类（样品）与类（样品）相聚的条件进行聚类，不满足条件者不得相聚。传统的一维、二维有序样品的聚类是这种聚类的特例，另外，方向数据、圆周上的样品、周期数据等的聚类亦可看作这种聚类的特例。并结合实例详细阐述了该法的步骤。},
  file = {/Users/isaac/Zotero/storage/32V2QQE5/陈 等。 - 1999 - 带约束的聚类分析.pdf},
  keywords = {带约束的聚类（条件系统聚类）,方向数据,有序样品,系统聚类},
  langid = {中文},
  number = {02}
}

@inproceedings{chengMusicStructureBoundary2018,
  title = {Music Structure Boundary Detection and Labelling by a Deconvolution of Path-Enhanced Self-Similarity Matrix},
  booktitle = {2018 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Cheng, Tian and Smith, Jordan BL and Goto, Masataka},
  date = {2018},
  pages = {106--110},
  publisher = {{IEEE}},
  file = {/Users/isaac/Zotero/storage/LY87MYSX/Cheng 等。 - 2018 - Music structure boundary detection and labelling b.pdf;/Users/isaac/Zotero/storage/XXAABTHQ/8461319.html}
}

@article{chenJiYuMFCCHeChangShuQBianHuanDeLeQiYinFuShiBie2020a,
  title = {基于MFCC和常数Q变换的乐器音符识别},
  author = {陈, 燕文 and 李, 坤 and 韩, 焱 and 王, 燕平},
  date = {2020},
  journaltitle = {计算机科学},
  volume = {47},
  pages = {149--155},
  issn = {1002-137X},
  url = {https://kns.cnki.net/KCMS/detail/detail.aspx?dbcode=CJFD&dbname=CJFDLAST2020&filename=JSJA202003026&v=},
  abstract = {音符识别是音乐信号分析处理领域内非常重要的研究内容,它为计算自动识谱、乐器调音、音乐数据库检索和电子音乐合成提供技术基础。传统的音符识别方法通过估计音符基频与标准频率进行一一对应识别。然而一一对应较为困难,且随着音符基频的增大将导致误差增大,可识别的音符基频范围不广。为此,文中采用分类的思想进行音符识别。首先,建立所需识别的音符音频库,并针对音乐信号低频信息的重要性,选取梅尔频率倒谱系数(Mel Frequency Cepstrum Coefficients,MFCC)和常数Q变换(Constant Q Transform,CQT)作为音符信号提取特征。然后,将提取的特征MFCC和CQT分别作为音符识别的单一特征输入和两者特征融合输入;结合Softmax回归模型在多分类问题中的优势以及BP神经网络良好的非线性映射能力与自学习能力,构建基于Softmax回归模型的BP神经网络多分类识别器。在MATLAB R2016a的仿真环境下,将特征参数输入到多分类器中进行学习与训练,通过调整网络参数来寻找最优解。通过改变训练样本数进行对比实验。实验结果表明,将融合特征(MFCC+CQT)作为特征输入时,可以识别出从大字组到小字三组的25类音符,并可以获得95.6\%的平均识别率;在识别过程中,特征CQT比特征MFCC的贡献更大。实验数据充分说明,利用分类的思想提取音符信号的MFCC和CQT特征来进行音符识别,可以取得很好的识别效果,并且不受音符基频范围的限制。},
  annotation = {{$<$}北大核心{$>$}},
  file = {/Users/isaac/Zotero/storage/KIEK33XU/基于MFCC和常数Q变换的乐器音符识别_陈燕文.pdf},
  keywords = {BP neural network,BP神经网络 Music note library,Constant Q transform,Feature fusion,Mel frequency cepstrum coefficients,MFCC,Softmax regression model,Softmax回归模型,常数Q变换,特征融合,音符库},
  langid = {中文;},
  number = {03}
}

@thesis{chenYinLeJieGouFenXiJiYingYong2006,
  title = {音乐结构分析及应用},
  author = {陈, 廷梁},
  date = {2006},
  institution = {{哈尔滨工业大学}},
  url = {https://kns.cnki.net/KCMS/detail/detail.aspx?dbcode=CMFD&dbname=CMFD0506&filename=2006171515.nh&v=},
  abstract = {音乐是音频的重要组成部分,它包含了丰富的语义信息。音乐结构是音乐的主要表现形式之一,它是理解音乐的重要途径。与音乐分析与检索相关的研究大部分都是基于结构化信息如音乐标签、MIDI格式、乐谱等,针对实际音乐内容的分析与检索的研究是最近三四年才出现的。本文针对实际音乐,从一个新的领域——音乐结构分析入手,对比分析以往研究的优点与不足,改进和设计一系列的新算法实现了音乐结构自动分析和标注,并以此为基础进一步研究了基于音乐结构的音乐检索和文摘。首先通过分析音乐的表达方式提取了PCP特征,这是一种基于帧的特征,它较好的结合了声学层的频率和音乐语义层的十二平均律信息。用这个特征和余弦距离来表达音乐片段之间的相似度。然后设计了一组源于编辑距离和动态规划思想的音乐结构分析算法,首先将特征向量分组,然后经过组相似匹配、组重现检测、重现组归并和自动标注四个前后衔接的环节实现了音乐结构的自动标注,较好地实现了将音频形式的音乐自动标注为表示音乐结构的三元组列表形式,由于这是一个新的领域,目前还没有比较好的量化评价方法,本文提出一种新的评价方法,并用它来评价结构分析的结果,取得了较好的效果。受到结构评价方法启发,本文又提出了基于结构分析的音乐检索的方法,初步探讨了利用音乐结构信息建立检索系统这一新方法的前景。初步实验结果显示这种方法是很有前途的,它可以直接检索音频形式的音乐,按照结构相似程度给出最后结果并排序输出,这在音乐检索和智能播放列表的生成方面都有广泛的应用。最后利用结构分析的结果在音乐文摘方面做了初步的尝试,目标是检出音乐作品的高潮部分,取得了76.67\%准确率,这也是音乐结构分析的一个重要应用。为了研究工作的展开和成果的实用化,我们还开发了一个音频分析平台。这是一个基于DirectShow的通用音视频格式解码系统,同时还具有音频分类和实时波形显示的功能,为算法实验和理论研究提供了实验平台。},
  editora = {李, 海峰},
  editoratype = {collaborator},
  file = {/Users/isaac/Zotero/storage/RUAN88VD/音乐结构分析及应用_陈廷梁.caj},
  keywords = {聚类,音乐结构分析},
  langid = {中文;},
  type = {硕士}
}

@article{dingYinLeXinHaoFenXiZhongChangShuQBianHuanDeXingNengYanJiu2005,
  title = {音乐信号分析中常数Q变换的性能研究},
  author = {丁, 志中 and 戴, 礼荣},
  date = {2005},
  journaltitle = {声学技术},
  pages = {259--263},
  issn = {1000-3630},
  url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CJFD&dbname=CJFD2005&filename=SXJS200504022&v=2jXjspO%25mmd2Bm7au1NJo2D%25mmd2FXL1dr0i16zE2j2wkqfho2IRNcUrvE5iqElIWqGBoOsvrO},
  urldate = {2021-03-11},
  abstract = {由于采用指数分布的频域采样点,常数Q变换(ConstantQTransform—CQT)在音乐信号的音调分析中有着独特的优点。但是常数Q变换的性能究竟如何,还需进行多方面的研究,以便能开发其潜力、完善其不足。对于两种CQT定义下的频率估计准确性问题进行了讨论,给出了频率估计误差的计算公式和给定音节频率分辨率下CQT窗函数长度应满足的关系;通过实验研究了CQT和DFT在多音阶频率检测时的性能以及窗函数对CQT性能的影响。通过和DFT的比较,可以看到CQT在某些方面具有比DFT更好的特性。对于CQT的一些局限性文中也作了简单的评述。},
  keywords = {constant Q transform,DFT,music signal processing,pitch detection,常数Q变换,音乐信号处理,音调提取},
  langid = {中文;},
  number = {04}
}

@article{dingYinLeXinHaoFenXiZhongChangShuQBianHuanDeXingNengYanJiu2005a,
  title = {音乐信号分析中常数Q变换的性能研究},
  author = {丁, 志中 and 戴, 礼荣},
  date = {2005},
  journaltitle = {声学技术},
  pages = {259--263},
  issn = {1000-3630},
  url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CJFD&dbname=CJFD2005&filename=SXJS200504022&v=2jXjspO%25mmd2Bm7au1NJo2D%25mmd2FXL1dr0i16zE2j2wkqfho2IRNcUrvE5iqElIWqGBoOsvrO},
  urldate = {2021-03-11},
  abstract = {由于采用指数分布的频域采样点,常数Q变换(ConstantQTransform—CQT)在音乐信号的音调分析中有着独特的优点。但是常数Q变换的性能究竟如何,还需进行多方面的研究,以便能开发其潜力、完善其不足。对于两种CQT定义下的频率估计准确性问题进行了讨论,给出了频率估计误差的计算公式和给定音节频率分辨率下CQT窗函数长度应满足的关系;通过实验研究了CQT和DFT在多音阶频率检测时的性能以及窗函数对CQT性能的影响。通过和DFT的比较,可以看到CQT在某些方面具有比DFT更好的特性。对于CQT的一些局限性文中也作了简单的评述。},
  keywords = {constant Q transform,DFT,music signal processing,pitch detection,常数Q变换,音乐信号处理,音调提取},
  langid = {中文;},
  number = {04}
}

@article{duanJiYuYinLeQingGanShiBieDeWuTaiDengGuangKongZhiFangFaYanJiu2020,
  title = {基于音乐情感识别的舞台灯光控制方法研究},
  author = {段, 中兴 and 严, 洁杰},
  date = {2020},
  journaltitle = {计算机测量与控制},
  volume = {28},
  pages = {95--100},
  issn = {1671-4598},
  url = {https://kns.cnki.net/KCMS/detail/detail.aspx?dbcode=CJFD&dbname=CJFDLAST2020&filename=JZCK202011020&v=},
  abstract = {为了实现音乐情感识别的舞台灯光自动控制,需对音乐文件进行情感标记;针对人工情感标记效率低、速度慢的问题,开展了基于音乐情感识别的舞台灯光控制方法研究,提出了一种基于支持向量机和粒子群优化的音乐情感特征提取、分类和识别算法;首先以231首MIDI音乐文件为例,对平均音高、平均音强、旋律的方向等7种音乐基本特征进行提取并进行标准化处理;之后组成音乐情感特征向量输入支持向量机(SVM)多分类器,并利用改进的粒子群算法(PSO)优化分类器参数,建立标准音乐分类模型;最后设计灯光动作模型,将新的音乐文件通过离散情感模型与灯光动作相匹配,生成舞台灯光控制方法;实验结果表明了情感识别模型的有效性,与传统SVM多分类模型相比,明显提高了音乐情感的识别率,减少了测试时间,从而为舞台灯光设计人员提供合理参考。},
  file = {/Users/isaac/Zotero/storage/4M272AN6/基于音乐情感识别的舞台灯光控制方法研究_段中兴.pdf},
  keywords = {MIDI,SVM,Thayer情感模型,粒子群算法,音乐情感识别},
  langid = {中文;},
  number = {11}
}

@inproceedings{footeAutomaticAudioSegmentation2000,
  title = {Automatic Audio Segmentation Using a Measure of Audio Novelty},
  booktitle = {2000 {{IEEE International Conference}} on {{Multimedia}} and {{Expo}}. {{ICME2000}}. {{Proceedings}}. {{Latest Advances}} in the {{Fast Changing World}} of {{Multimedia}} ({{Cat}}. {{No}}. {{00TH8532}})},
  author = {Foote, Jonathan},
  date = {2000},
  volume = {1},
  pages = {452--455},
  publisher = {{IEEE}},
  file = {/Users/isaac/Zotero/storage/QEFUNEAY/Foote - 2000 - Automatic audio segmentation using a measure of au.pdf;/Users/isaac/Zotero/storage/AZGAF4Q2/869637.html}
}

@inproceedings{guoEEGbasedEmotionClassification2017,
  title = {{{EEG}}-Based Emotion Classification Using Innovative Features and Combined {{SVM}} and {{HMM}} Classifier},
  booktitle = {2017 39th {{Annual International Conference}} of the {{IEEE Engineering}} in {{Medicine}} and {{Biology Society}} ({{EMBC}})},
  author = {Guo, Kairui and Candra, Henry and Yu, Hairong and Li, Huiqi and Nguyen, Hung T. and Su, Steven W.},
  date = {2017},
  pages = {489--492},
  publisher = {{IEEE}},
  file = {/Users/isaac/Zotero/storage/BRRNBNVR/Guo 等。 - 2017 - EEG-based emotion classification using innovative .pdf;/Users/isaac/Zotero/storage/BFRDCM4P/8036868.html},
  keywords = {HMM,SVM,音乐情感识别}
}

@article{levyStructuralSegmentationMusical2008,
  title = {Structural {{Segmentation}} of {{Musical Audio}} by {{Constrained Clustering}}},
  author = {Levy, M. and Sandler, M.},
  date = {2008},
  journaltitle = {IEEE Transactions on Audio Speech \& Language Processing},
  volume = {16},
  pages = {318--326},
  doi = {10.1109/TASL.2007.910781},
  url = {http://www.researchgate.net/publication/3458007_Structural_Segmentation_of_Musical_Audio_by_Constrained_Clustering},
  urldate = {2021-01-11},
  abstract = {我们描述了一种基于光谱特征分层标签将音乐音频分割成结构部分的方法。音频帧首先使用基于特征训练的隐藏马尔可夫模型被标记为属于许多离散状态之一。然后，使用聚类算法将相邻帧的直方图聚类为表示状态不同分布的段类型，其中时间连续性表示为一组由隐藏的马尔可夫随机场建模的约束。我们给出的实验结果表明，在许多情况下，所产生的分割与传统的音乐形式概念非常吻合。我们进一步展示了如何轻松地扩展约束聚类方法，以包括先前的音乐知识、来自其他机器方法的输入或半监督。},
  file = {/Users/isaac/Zotero/storage/ADGYEJC7/Levy 和 Sandler - 2008 - Structural Segmentation of Musical Audio by Constr.pdf},
  keywords = {约束聚类,聚类,音乐结构},
  number = {2}
}

@article{liuYinLeTeZhengShiBieDeYanJiuZongShu2002,
  title = {音乐特征识别的研究综述},
  author = {刘, 丹 and 张, 乃尧 and 朱, 汉城},
  date = {2002},
  journaltitle = {计算机工程与应用},
  pages = {74--77},
  issn = {1002-8331},
  url = {https://kns.cnki.net/KCMS/detail/detail.aspx?dbcode=CJFD&dbname=CJFD2002&filename=JSGG200224023&v=},
  urldate = {2021-01-11},
  abstract = {文章全面总结了音乐特征识别领域所取得的主要研究成果,重点介绍了音乐特征的提取、描述、分析和识别等方面采用的各种智能分析处理方法,并对该领域中存在的主要困难和将来的发展方向提出了一些看法。},
  file = {/Users/isaac/Zotero/storage/JYDDST5V/刘 等。 - 2002 - 音乐特征识别的研究综述.pdf},
  keywords = {专家系统,人工智能,模糊系统,神经网络,音乐特征识别},
  langid = {中文;},
  number = {24}
}

@inproceedings{luRepeatingPatternDiscovery2004,
  title = {Repeating Pattern Discovery and Structure Analysis from Acoustic Music Data},
  booktitle = {Proceedings of the 6th {{ACM SIGMM}} International Workshop on {{Multimedia}} Information Retrieval},
  author = {Lu, Lie and Wang, Muyuan and Zhang, Hong-Jiang},
  date = {2004},
  pages = {275--282},
  file = {/Users/isaac/Zotero/storage/ND37B6AJ/Lu 等。 - 2004 - Repeating pattern discovery and structure analysis.pdf;/Users/isaac/Zotero/storage/6K4J5D2D/1026711.html},
  keywords = {自相关相似矩阵,重复片段,音乐结构分析}
}

@inproceedings{maddageContentbasedMusicStructure2004,
  title = {Content-Based Music Structure Analysis with Applications to Music Semantics Understanding},
  booktitle = {Proceedings of the 12th Annual {{ACM}} International Conference on {{Multimedia}}},
  author = {Maddage, Namunu C. and Xu, Changsheng and Kankanhalli, Mohan S. and Shao, Xi},
  date = {2004},
  pages = {112--119},
  file = {/Users/isaac/Zotero/storage/P2YYCN6U/Maddage 等。 - 2004 - Content-based music structure analysis with applic.pdf;/Users/isaac/Zotero/storage/69KZH63I/1027527.html}
}

@inproceedings{mauchUsingMusicalStructure2009,
  title = {Using {{Musical Structure}} to {{Enhance Automatic Chord Transcription}}.},
  booktitle = {{{ISMIR}}},
  author = {Mauch, Matthias and Noland, Katy C. and Dixon, Simon},
  date = {2009},
  pages = {231--236},
  file = {/Users/isaac/Zotero/storage/BIFP3VWP/Mauch 等。 - 2009 - Using Musical Structure to Enhance Automatic Chord.pdf},
  keywords = {音乐结构分析}
}

@inproceedings{paulusMusicStructureAnalysis2006,
  title = {Music Structure Analysis by Finding Repeated Parts},
  booktitle = {Proceedings of the 1st {{ACM}} Workshop on {{Audio}} and Music Computing Multimedia},
  author = {Paulus, Jouni and Klapuri, Anssi},
  date = {2006},
  pages = {59--68},
  file = {/Users/isaac/Zotero/storage/NMRNC42D/Paulus 和 Klapuri - 2006 - Music structure analysis by finding repeated parts.pdf;/Users/isaac/Zotero/storage/2WYDF2NE/1178723.html}
}

@inproceedings{shiuSimilarityMatrixProcessing2006,
  title = {Similarity Matrix Processing for Music Structure Analysis},
  booktitle = {Proceedings of the 1st {{ACM}} Workshop on {{Audio}} and Music Computing Multimedia},
  author = {Shiu, Yu and Jeong, Hong and Kuo, C.-C. Jay},
  date = {2006},
  pages = {69--76},
  file = {/Users/isaac/Zotero/storage/25E7D8GT/Shiu 等。 - 2006 - Similarity matrix processing for music structure a.pdf;/Users/isaac/Zotero/storage/H7UPINRA/1178723.html},
  keywords = {自相关相似矩阵,音乐结构分析}
}

@online{ShiZhanXiangMuPythonShiXianBenDiYinLeBoFangQi,
  title = {实战项目 —python实现本地音乐播放器},
  url = {https://zhuanlan.zhihu.com/p/40379098},
  urldate = {2021-03-07},
  abstract = {随着网络的发展，我们已经很少将音乐下载到本地，而是直接在线听歌，方便而又直接。也许你用的音乐播放器是这个 也许是这个 这都不是重点，今天我们要用python自己打造一款音乐播放器。 具体思路使用python制作一…},
  file = {/Users/isaac/Zotero/storage/S5JWHZ5L/40379098.html},
  langid = {pinyin},
  organization = {{知乎专栏}}
}

@article{songEVALUATIONMUSICALFEATURES2012,
  title = {{{EVALUATION OF MUSICAL FEATURES FOR EMOTION CLASSIFICATION}}},
  author = {Song, Yading and Dixon, Simon and Pearce, Marcus},
  date = {2012},
  pages = {6},
  abstract = {Because music conveys and evokes feelings, a wealth of research has been performed on music emotion recognition. Previous research has shown that musical mood is linked to features based on rhythm, timbre, spectrum and lyrics. For example, sad music correlates with slow tempo, while happy music is generally faster. However, only limited success has been obtained in learning automatic classifiers of emotion in music. In this paper, we collect a ground truth data set of 2904 songs that have been tagged with one of the four words “happy”, “sad”, “angry” and “relaxed”, on the Last.FM web site. An excerpt of the audio is then retrieved from 7Digital.com, and various sets of audio features are extracted using standard algorithms. Two classifiers are trained using support vector machines with the polynomial and radial basis function kernels, and these are tested with 10-fold cross validation. Our results show that spectral features outperform those based on rhythm, dynamics, and, to a lesser extent, harmony. We also find that the polynomial kernel gives better results than the radial basis function, and that the fusion of different feature sets does not always lead to improved classification.},
  file = {/Users/isaac/Zotero/storage/J2K5JZE7/Song 等。 - 2012 - EVALUATION OF MUSICAL FEATURES FOR EMOTION CLASSIF.pdf},
  keywords = {SVM,音乐情感识别},
  langid = {english}
}

@article{spleeter2020,
  title = {Spleeter: A Fast and Efficient Music Source Separation Tool with Pre-Trained Models},
  author = {Hennequin, Romain and Khlif, Anis and Voituret, Felix and Moussallam, Manuel},
  date = {2020},
  journaltitle = {Journal of Open Source Software},
  volume = {5},
  pages = {2154},
  publisher = {{The Open Journal}},
  doi = {10.21105/joss.02154},
  url = {https://doi.org/10.21105/joss.02154},
  number = {50}
}

@inproceedings{tralieEnhancedHierarchicalMusic2019,
  title = {Enhanced Hierarchical Music Structure Annotations via Feature Level Similarity Fusion},
  booktitle = {{{ICASSP}} 2019-2019 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Tralie, Christopher J. and McFee, Brian},
  date = {2019},
  pages = {201--205},
  publisher = {{IEEE}},
  file = {/Users/isaac/Zotero/storage/Q5LYE4WZ/Tralie 和 McFee - 2019 - Enhanced hierarchical music structure annotations .pdf;/Users/isaac/Zotero/storage/HPYLXFD5/8683492.html}
}

@article{vanbovenMusicStructureAnalysis,
  title = {On {{Music Structure Analysis}}},
  author = {van Boven, Leander},
  pages = {71},
  abstract = {This thesis proposes a novel approach to Music Structure Analysis (MSA). This approach implements the Segmentation by Annotation (SbA) approach to MSA, using a convolutional neural network (CNN) and an artificial neural network using Long Short-Term Memory (LSTM) units. An overview of the current advances in music structure analysis is given as well as the use of the proposed architectures in similar research fields. A description of the evaluation methods is provided in which the proposed architectures show promising results on the custom ground truth used. This custom ground truth is a modified version of the humanly annotated segments found in the internet archives subset of the SALAMI dataset. The ground truth is modified by reducing the amount of unique high-level segment functions from 26 to 9. By comparing the SbA approach to the (more symbolic) Distance-based Segmentation and Annotation approach, a comparison between using machine learning and non-machine learning techniques can be made. Future research is proposed to enhance the segmentation by annotation approach as well as music structure analysis in general.},
  file = {/Users/isaac/Zotero/storage/8B5IQHUI/van Boven - On Music Structure Analysis.pdf},
  keywords = {模板匹配,自相关相似矩阵,音乐结构分析},
  langid = {english},
  options = {useprefix=true}
}

@book{wudazhengXinHaoYuXianXingXiTongFenXi2005,
  title = {信号与线性系统分析},
  author = {吴大正},
  date = {2005-08},
  publisher = {{高等教育}},
  url = {https://book.douban.com/subject/1679604/},
  urldate = {2021-03-10},
  isbn = {978-7-04-017401-4},
  keywords = {专业,信号与系统,大学教材,教材,电子,课本},
  pagetotal = {452}
}

@thesis{wuJiYuYinLeQingGanQuDongDeWuTaiDengGuangKongZhiYanJiu2014,
  title = {基于音乐情感驱动的舞台灯光控制研究},
  author = {武, 宇},
  date = {2014},
  institution = {{北京交通大学}},
  url = {https://kns.cnki.net/KCMS/detail/detail.aspx?dbcode=CMFD&dbname=CMFD201501&filename=1014400175.nh&v=},
  abstract = {摘要：基于音乐情感的舞台灯光控制是通过计算机模拟人脑对音乐的情感认知和创作智能,来实现舞台灯光的智能化控制。本文通过对音乐特征提取方法和音乐情感识别技术的分析和研究,设计了一种基于音乐情感驱动的舞台灯光控制。旨在通过计算机提取音乐的基本特征来识别音乐的情感内涵,从而设计出符合特定情感内涵的舞台灯光表演方案。论文研究了音乐特征提取方法。在探讨MIDI音乐格式的结构和基本特征的提取方法,以及分析基本特征的情感内涵的基础上,通过Matlab编程提取了音高、音强、速度等基本特征,并通过乐理计算,构建了能够表征音乐情感内涵的特征矢量模型。研究了音乐情感识别技术。在Hevner情感分类模型的基础上,研究了基于"IF-THEN规则”的模糊推理逻辑,将特征矢量模型中的基本特征作为输入、将音乐情感分类和灯光动作作为输出,设计了基于Mamdani的多输入、多输出控制的音乐情感驱动平台,实现了对音乐情感的准确识别。研究了舞台灯光的动作设计。通过对灯光情感内涵和灯光动作,以及音乐情感和灯光动作的同步研究,设计了一种根据音乐情感基本特征要素的实时控制灯光动作控制方案,设计了灯光动作库。对音乐情感驱动进行了仿真。搭建了基于Matlab的仿真平台,利用灯光控制软件FreeStyler和EasyView搭建了模拟舞台,分别对若干乐曲的情感识别情况和灯光动作演示效果进行了仿真和评估,并通过实际舞台应用验证了该平台的可行性。该平台具有操作简单、设计周期短、效率高、成本低等优点,能够快速高效的设计出符合特定音乐情感的舞台灯光表演方案,具有较强的实用价值。},
  editora = {魏, 学业},
  editoratype = {collaborator},
  file = {/Users/isaac/Zotero/storage/ZSPFXQW9/基于音乐情感驱动的舞台灯光控制研究_武宇.caj},
  keywords = {Hevner情感模型,MIDI,模糊推理,音乐特征提取},
  langid = {中文;},
  type = {硕士}
}

@book{wuzuqiangQuShiYuZuoPinFenXi2003,
  title = {曲式与作品分析},
  author = {吴祖强},
  date = {2003-06-01},
  publisher = {{人民音乐出版社}},
  url = {https://book.douban.com/subject/1134132/},
  urldate = {2021-02-24},
  isbn = {978-7-103-02719-6},
  keywords = {作曲,吴祖强,教材,曲式分析,艺术工具,艺术理论,音乐,音乐理论},
  pagetotal = {416}
}

@article{zhangJiYuYangBenDeLiuXingGeQuGuanJianDuanFenGeFangFa2006,
  title = {基于样本的流行歌曲关键段分割方法},
  author = {张, 一彬 and 周, 杰 and 边, 肇祺},
  date = {2006},
  journaltitle = {电子学报},
  pages = {220--225},
  issn = {0372-2112},
  url = {https://kns.cnki.net/kcms/detail/frame/list.aspx?dbcode=CJFQ&filename=dzxu200602006&dbname=CJFD2006&RefType=3&vl=Zk5XzRKvkmw%mmd2FkHNTd9ETqapAXQvlsT8iP4wxM2eCNq6r5mbALVcsy9xJd6mcHImS},
  urldate = {2021-02-25},
  abstract = {流行歌曲的关键段为歌曲中最能打动人、给人印象最深刻的一个完整片段.将它分割出来可用于音乐试听和基于内容的音乐分类、检索、管理.通过对人工截取的样本进行分析,本文提出了一种流行歌曲关键段自动分割方法.实验结果表明,此方法可以比较准确和有效地分割出流行歌曲中的关键段.},
  file = {/Users/isaac/Zotero/storage/C6VMTJAH/基于样本的流行歌曲关键段分割方法_张一彬.caj},
  keywords = {automatic music summary,digital entertainment,neural network,popular song,数字娱乐,流行歌曲,神经网络,音乐自动摘要},
  langid = {中文;},
  number = {02}
}

@online{zhengsongzhiZhuChengFenFenXiPCAJiQiMATLABDeShiXianFangFa2020,
  title = {主成分分析（{{PCA}}）{{及其MATLAB的实现方法}}},
  author = {, 郑淞之},
  date = {2020-02-23},
  url = {https://blog.csdn.net/szzheng/article/details/104451760},
  urldate = {2021-01-14}
}

@online{zhiyipeiyinYinPinYouJiZhongGeShi,
  title = {音频有几种格式？},
  author = {知意配音},
  url = {https://www.zhihu.com/question/386007403/answer/1160498210}
}


